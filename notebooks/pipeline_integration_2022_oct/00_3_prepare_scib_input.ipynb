{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe9f8e0-2618-43c5-b3b7-f9c6e2478f1d",
   "metadata": {},
   "source": [
    "### To execute the contents of this notebook as a python script, please execute\n",
    "`./run_ipynb_w_python.sh 00_3_prepare_scib_input`\n",
    "\n",
    "### This routine needs up to 200GB of RAM to concatenate the largest objects. For that reason, it is recommended to execute as a cluster command with higher memory.\n",
    "`sbatch submit_00_3_prepare_scib_input.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56e6e185",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3ed9e1a-4408-43f5-8c6b-ff8db5940940",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "log_dataset_k = 'integration_oct_2022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5536e3a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/ignacio.ibarra/Dropbox/workspace/theislab/retina/notebooks/pipeline_integration_2022_oct\n"
     ]
    }
   ],
   "source": [
    "cd ~/workspace/theislab/retina/notebooks/pipeline_integration_2022_oct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30cdcb52",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import scanpy as sc\n",
    "from os.path import join, exists\n",
    "from os import listdir\n",
    "import anndata\n",
    "import scipy\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import utils\n",
    "\n",
    "# convert counts into float32\n",
    "# Convenience method for computing the size of objects\n",
    "def print_size_in_MB(x):\n",
    "    return '{:.3} MB'.format(x.__sizeof__()/1e6)\n",
    "\n",
    "def print_size_in_MB_sparse_matrix(a):\n",
    "    # a = scipy.sparse.csr_matrix(np.random.randint(10, size=(40, 3)))\n",
    "    # x = a.data.nbytes + a.indptr.nbytes + a.indices.nbytes\n",
    "    size = a.data.size/(1024**2)\n",
    "    return '{:.3} MB'.format(size)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ea146a9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all ['Chen_a', 'Chen_rgc', 'Chen_b', 'Chen_c', 'Chang', 'Hackney', 'Roska', 'Hafler', 'Wong', 'Scheetz', 'Sanes']\n"
     ]
    }
   ],
   "source": [
    "combinations = [['Chen_a', 'Chen_rgc', 'Chen_b', 'Chen_c', 'Chang', 'Hackney', 'Roska', 'Hafler', 'Wong', 'Scheetz', 'Sanes'],\n",
    "                ['Chen_b', 'Chen_c', 'Chen_a']]\n",
    "    \n",
    "dataset_codes = ['all', 'Chen']\n",
    "# add Chen_a plus all others\n",
    "for k in combinations[0]:\n",
    "    if 'Chen' in k:\n",
    "        continue\n",
    "    combinations.append(combinations[1] + [k])\n",
    "    dataset_codes.append('Chen+%s' % k)\n",
    "\n",
    "combinations = combinations[:1]\n",
    "dataset_codes = dataset_codes[:1]\n",
    "\n",
    "for x, y in zip(dataset_codes, combinations):\n",
    "    print(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6654aac4-1752-46a4-ac1c-ff16130d9439",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# combinations = [[['Hackney', 'Roska']], # ['Hackney', 'Roska', 'Hafler', 'Wong', 'Scheetz', 'Chen_b', 'Chen_c', 'Sanes', 'Chen_a'],\n",
    "#                 ['Chen_a', 'Chen_b', 'Chen_c']]\n",
    "    \n",
    "# dataset_codes = ['all', 'Chen']\n",
    "# # add Chen_a plus all others\n",
    "# for k in combinations:\n",
    "#     # if not 'Chen' in k:\n",
    "#     #     continue\n",
    "#     combinations.append(combinations[1] + k)\n",
    "#     dataset_codes.append('Chen+%s' % '+'.join(k))\n",
    "\n",
    "# # print(dataset_codes)\n",
    "\n",
    "# combinations = combinations[-1:]\n",
    "# dataset_codes = dataset_codes[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14ba7e50-28a1-414e-ab6e-7c60dae7637f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e690682-f68e-4b96-b5f4-14253e466cd6",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd012d90-5775-4396-a979-3bea524d1754",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# add donor information\n",
    "path_xlsx = '/home/theislab/l_ibarra/workspace/theislab/retina/data/RNA/atlasrna_metadata.xlsx'\n",
    "xl = pd.ExcelFile(path_xlsx)\n",
    "xl.sheet_names  # see all sheet names\n",
    "\n",
    "donor = []\n",
    "for sheet_name in xl.sheet_names:\n",
    "    df2 = xl.parse(sheet_name)  # read a specific sheet to DataFrame\n",
    "    # print(sheet_name, df.shape)\n",
    "    df2['sheet_name'] = sheet_name\n",
    "    donor.append(df2)\n",
    "donor = pd.concat(donor).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38389ae7",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datadir = '/mnt/f/workspace/theislab/retina/data/%s/input' % log_dataset_k\n",
    "datadir = '/home/theislab/l_ibarra/workspace/theislab/retina/data/%s/input/' % log_dataset_k\n",
    "os.path.exists(datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e9d52f5",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_500\n",
      "# of cells (input argument) 500 (None = all cells\n",
      "True /mnt/f/workspace/theislab/retina/data/integration_oct_2022/input/input_500_cells_all_sn.h5ad\n",
      "_500\n",
      "# of cells (input argument) 500 (None = all cells\n",
      "True /mnt/f/workspace/theislab/retina/data/integration_oct_2022/input/input_500_cells_all_sc.h5ad\n",
      "_500\n",
      "# of cells (input argument) 500 (None = all cells\n",
      "True /mnt/f/workspace/theislab/retina/data/integration_oct_2022/input/input_500_cells_all_sn+sc.h5ad\n"
     ]
    }
   ],
   "source": [
    "overwrite = False\n",
    "\n",
    "tech_groups = {'sn': {'sn'}, 'sc': {'sc'}, 'sn+sc': {'sn', 'sc'}}\n",
    "for n_sample_per_batch in [500,]: #  500, None,]: # [100, 500, 750, 1000, 1500, 2000]: # , 500, None]:\n",
    "    for tech_group in tech_groups:\n",
    "        # if n_sample_per_batch != None:\n",
    "        #    continue\n",
    "        # if n_sample_per_batch != None and n_sample_per_batch != 500:\n",
    "        #     continue\n",
    "        # examine types, columns and others incorporated in the object\n",
    "\n",
    "        code_n_cells = (('_' + str(n_sample_per_batch) if n_sample_per_batch is not None else ''))\n",
    "\n",
    "        print(code_n_cells)\n",
    "\n",
    "        print('# of cells (input argument)', n_sample_per_batch, '(None = all cells')\n",
    "\n",
    "        code_output = (('_' + str(n_sample_per_batch) if n_sample_per_batch is not None else '_all'))\n",
    "\n",
    "        for dataset_names_subset, dataset_code in zip(combinations, dataset_codes):\n",
    "\n",
    "            output_path = os.path.join(datadir, 'input%s_cells_%s_%s.h5ad') % (code_output, dataset_code, tech_group)\n",
    "            print(exists(output_path), output_path)\n",
    "\n",
    "            # assert False\n",
    "            if exists(output_path):\n",
    "                continue\n",
    "\n",
    "            # print(dataset_code, dataset_names_subset)\n",
    "            p1 = output_path.replace('.h5ad', '_part1.h5ad')\n",
    "\n",
    "            print(dataset_names_subset)\n",
    "            names1 = dataset_names_subset[:1]\n",
    "            names2 = dataset_names_subset[1:4]\n",
    "            names3 = dataset_names_subset[3:]\n",
    "\n",
    "            # if dataset_code != 'all':\n",
    "            #     names1, names2, names3 = names1, [], []\n",
    "            print('names1', names1)\n",
    "            print('names2', names2)\n",
    "            print('names3', names3)\n",
    "\n",
    "            # assert False\n",
    "\n",
    "            if not exists(p1) and len(names1) > 0:\n",
    "                ad1 = utils.get_datasets(names1, code_n_cells=code_n_cells, dataset_code=log_dataset_k)\n",
    "\n",
    "                print('ad1')\n",
    "                print ('laoding datasets 1 done...')\n",
    "                print(ad1.obs.dataset.value_counts())\n",
    "                # save part1\n",
    "                 # save part1\n",
    "                ad1 = ad1[ad1.obs.dataset.isin(set(names1)),:]\n",
    "                ad1.write(p1, compression='lzf')\n",
    "                del ad1\n",
    "                print(p1)\n",
    "\n",
    "            p2 = output_path.replace('.h5ad', '_part2.h5ad')\n",
    "            if not exists(p2) and len(names2) > 0:\n",
    "                print('loading', names2)\n",
    "                ad2 = utils.get_datasets(names2, code_n_cells=code_n_cells, dataset_code=log_dataset_k)\n",
    "                print('ad2')\n",
    "                print(ad2)\n",
    "                print(ad2.obs.index)\n",
    "                print ('laoding datasets 2 done...')\n",
    "                print(ad2.obs.dataset.value_counts())\n",
    "\n",
    "                # save part1\n",
    "                ad2 = ad2[ad2.obs.dataset.isin(set(names2)),:]\n",
    "                ad2.write(p2, compression='lzf')\n",
    "                del ad2\n",
    "                print(p2)   \n",
    "\n",
    "            p3 = output_path.replace('.h5ad', '_part3.h5ad')\n",
    "            if not exists(p3) and len(names3) > 0:\n",
    "                print('loading', names3)\n",
    "                ad3 = utils.get_datasets(names3, code_n_cells=code_n_cells, dataset_code=log_dataset_k)\n",
    "                print('ad2')\n",
    "                print(ad3)\n",
    "                print(ad3.obs.index)\n",
    "                print ('laoding datasets 3 done...')\n",
    "                print(ad3.obs.dataset.value_counts())\n",
    "\n",
    "                ad3 = ad3[ad3.obs.dataset.isin(set(names3)),:]\n",
    "                ad3.write(p3, compression='lzf')\n",
    "                del ad3\n",
    "                print(p3)    \n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            ad1, ad2, ad3 = None, None, None\n",
    "            # filter: only the datasets subset can be in the object\n",
    "            ad1 = sc.read_h5ad(p1) #  cache=True)\n",
    "            ad1 = ad1[ad1.obs['dataset'].isin(set(dataset_names_subset))]\n",
    "            print(ad1.obs.dataset.value_counts())\n",
    "\n",
    "            if exists(p2):\n",
    "                ad2 = sc.read_h5ad(p2) #  cache=True)\n",
    "                ad2 = ad2[ad2.obs['dataset'].isin(set(dataset_names_subset))]\n",
    "                print(ad2.obs.dataset.value_counts())\n",
    "\n",
    "            gc.collect()\n",
    "            print('concatenating...')\n",
    "            print(p1)\n",
    "            print(p2)\n",
    "            print(p3, 'pending ad1/ad2 concatenation')\n",
    "\n",
    "            print('concatenating ad1/ad2')\n",
    "            print(ad1.shape)\n",
    "            print(ad2.shape)  \n",
    "            \n",
    "            ad1.layers['counts'] = ad1.layers['counts'].astype('int16')\n",
    "            ad2.layers['counts'] = ad2.layers['counts'].astype('int16')\n",
    "            gc.collect()\n",
    "            ad_final = anndata.concat([ad1, ad2]) if (ad2 is not None) else ad1\n",
    "            if ad1 is not None:\n",
    "                del ad1\n",
    "            if ad2 is not None:\n",
    "                del ad2\n",
    "            gc.collect()        \n",
    "\n",
    "            if exists(p3):\n",
    "                ad3 = sc.read_h5ad(p3) #  cache=True)\n",
    "                ad3 = ad3[ad3.obs['dataset'].isin(set(dataset_names_subset))]\n",
    "                print(ad3.obs.dataset.value_counts())\n",
    "                # print(ad2.obs.dataset.value_counts())\n",
    "\n",
    "            ad3.layers['counts'] = ad3.layers['counts'].astype('int16')\n",
    "            gc.collect()\n",
    "            \n",
    "            print('concatenating ad_final/ad3')\n",
    "            ad_final = anndata.concat([ad_final, ad3]) if (ad3 is not None) else ad_final\n",
    "\n",
    "            # keep a log of the technogy\n",
    "            ad_final.obs['tech'] = np.where(ad_final.obs['dataset'].astype(str).str.split('_').str[0].isin({'Chen', 'Hackney'}), 'sn', 'sc')\n",
    "            if ad3 is not None:\n",
    "                del ad3\n",
    "                \n",
    "            ad_final = ad_final[ad_final.obs['tech'].isin(tech_groups[tech_group])]\n",
    "            print(ad_final.obs.groupby(['dataset', 'tech']).size())\n",
    "            # print(ad1.shape, ad2.shape, ad3.shape)\n",
    "\n",
    "            gc.collect()\n",
    "            print('done...')\n",
    "\n",
    "            print('ad final')\n",
    "            # print(ad1.shape, ad2.shape)\n",
    "            print(ad_final.shape)\n",
    "            # print(ad_final.obs.index)\n",
    "\n",
    "            # define a unified code for all categories\n",
    "            ad_final.obs['batch.merged'] = ad_final.obs['dataset'].astype(str) + ':' + ad_final.obs['batch'].astype(str)\n",
    "            ad_final.obs['batch.merged'] = ad_final.obs['batch.merged'].astype('category').cat.codes\n",
    "            # input_scib.obs['batch.merged'].value_counts()\n",
    "            ad_final.obs['batch.merged'] = ad_final.obs['batch.merged'].astype('category').astype(str)\n",
    "            # print(ad_final.obs['batch.merged'].value_counts())\n",
    "\n",
    "            # include the donor information using the metadata path from \n",
    "            donor_by_filename = donor[['sampleid', 'donor']].set_index('sampleid')['donor'].to_dict()\n",
    "            ad_final.obs['donor'] = ad_final.obs['filename'].map(donor_by_filename).astype(str)\n",
    "\n",
    "            ad_final.obs['batch_donor_dataset'] = ad_final.obs['donor'].astype(str) + ':' + ad_final.obs['dataset'].astype(str) + ':' + ad_final.obs['batch'].astype(str)\n",
    "\n",
    "            print('before batch filter (n=100)')\n",
    "            print(ad_final.shape)\n",
    "            ad_final = ad_final[ad_final.obs['batch_donor_dataset'].map(ad_final.obs['batch_donor_dataset'].value_counts().to_dict()) > 100,:]\n",
    "            ad_final.obs['batch_donor_dataset'].value_counts()\n",
    "\n",
    "            print(ad_final.obs.dataset.value_counts())\n",
    "\n",
    "            ad_final.obs['batch_donor_dataset'] = ad_final.obs['batch_donor_dataset'].astype('category')\n",
    "\n",
    "            print('after batch filter (n=100)')\n",
    "            print(ad_final.shape)\n",
    "            print('saving to output...')       \n",
    "\n",
    "            # convert counts into int16\n",
    "            ad_final.layers['counts'] = ad_final.layers['counts'].astype('int16')\n",
    "            \n",
    "            ad_final.write(output_path, compression='lzf')\n",
    "\n",
    "            if exists(p1):\n",
    "                os.remove(p1)\n",
    "            if exists(p2):\n",
    "                os.remove(p2)\n",
    "            if exists(p3):\n",
    "                os.remove(p3)\n",
    "\n",
    "            print('done...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e9e1ec-09f9-41bc-9fbd-8c5d79d1cb2e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scIB-python]",
   "language": "python",
   "name": "conda-env-scIB-python-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
