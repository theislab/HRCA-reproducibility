{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason the import of scgen in cluster fails if anndata is not imported first.\n",
    "import anndata\n",
    "import scgen\n",
    "scgen.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scIB\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "# ls -ltrh /storage/groups/ml01/workspace/ignacio.ibarra/theislab/retinal_scRNAseq_integration/data/integration_march_2021/input/input*_cells.h5ad\n",
    "import os\n",
    "from os.path import exists, join\n",
    "from os import makedirs\n",
    "import pandas as pd\n",
    "# ad = sc.read_h5ad('/home/icb/ignacio.ibarra/theislab/scIB_output/retinal_all_batch_donor_dataset_cell.type/prepare/unscaled/HVG.2K/adata_pre.h5ad')\n",
    "# ad.shape\n",
    "### here we write a version of the script in where we are sampling Chen_a/b/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/icb/ignacio.ibarra/miniconda3/envs/scIB-python/lib/python3.7/site-packages/ipykernel_launcher.py', '-f', '/home/icb/ignacio.ibarra/jupyter/lisa/runtime/kernel-0b0ec224-f3d5-45ed-a37e-1793eb826e6b.json']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.argv)\n",
    "query = sys.argv[1] # 'heart_ventricle' # 'heart_ventricle', 'heart_atrial'\n",
    "\n",
    "if query == '-f':\n",
    "    query = 0\n",
    "else:\n",
    "    query = int(query)\n",
    "\n",
    "# read queries from dataframe path\n",
    "# df = pd.read_csv('queries_gpu_methods.csv', index_col=0)\n",
    "df = pd.read_csv('queries_gpu_methods.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What really works seems to be having the tf-nightly installation with latest keras, and modifying scgen to allow tf.keras.optimizers\n",
    "# pip install tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch + cuda\n",
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('pytorch + cuda')\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior() \n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scgen\n",
      "# of detected devices 1\n",
      "0 /device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "print('scgen')\n",
    "from tensorflow.python.client import device_lib\n",
    "devices = device_lib.list_local_devices()\n",
    "print('# of detected devices', len(devices))\n",
    "for di, device in enumerate(devices):\n",
    "    print(di, device.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False /storage/groups/ml01/workspace/ignacio.ibarra/theislab/retinal_scRNAseq_integration/data/integration_march_2021/output/retinal_all_Chen+Hackney_batch_donor_dataset_cell.type/integration/unscaled/HVG.3K/scgen_E50.h5ad\n",
      "False /storage/groups/ml01/workspace/ignacio.ibarra/theislab/retinal_scRNAseq_integration/data/integration_march_2021/output/retinal_all_Chen+Hackney_batch_donor_dataset_cell.type/integration/unscaled/HVG.3K/scgen_E100.h5ad\n",
      "False /storage/groups/ml01/workspace/ignacio.ibarra/theislab/retinal_scRNAseq_integration/data/integration_march_2021/output/retinal_all_Chen+Hackney_batch_donor_dataset_cell.type/integration/unscaled/HVG.3K/scgen_E25.h5ad\n"
     ]
    }
   ],
   "source": [
    "# # print(df)\n",
    "# for p in  df['input']:\n",
    "#     if not exists(p):\n",
    "#         print(exists(p), p)\n",
    "# for p in  df['output']:\n",
    "#     if not exists(p):\n",
    "#         print(exists(p), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to make umap\n",
    "def reduce_data(adata, batch_key=None, subset=False,\n",
    "                filter=True, flavor='cell_ranger', n_top_genes=2000, n_bins=20,\n",
    "                pca=True, pca_comps=50, overwrite_hvg=True,\n",
    "                neighbors=True, use_rep='X_pca', \n",
    "                umap=True):\n",
    "    \"\"\"\n",
    "    overwrite_hvg:\n",
    "        if True, ignores any pre-existing 'highly_variable' column in adata.var\n",
    "        and recomputes it if `n_top_genes` is specified else calls PCA on full features.\n",
    "        if False, skips HVG computation even if `n_top_genes` is specified and uses\n",
    "        pre-existing HVG column for PCA\n",
    "    \"\"\"\n",
    "    \n",
    "    print('setting up HVGs')\n",
    "    assert n_top_genes == adata.shape[1]\n",
    "    adata.var[\"highly_variable\"] = True\n",
    "    n_hvg = np.sum(adata.var[\"highly_variable\"])\n",
    "        \n",
    "    print('PCA and/or neighbors')\n",
    "    if pca:\n",
    "        print(\"PCA\")\n",
    "        use_hvgs = not overwrite_hvg and \"highly_variable\" in adata.var\n",
    "        sc.tl.pca(adata,\n",
    "                  n_comps=pca_comps, \n",
    "                  use_highly_variable=use_hvgs, \n",
    "                  svd_solver='arpack', \n",
    "                  return_info=True)\n",
    "    \n",
    "    if neighbors:\n",
    "        n_jobs = 4\n",
    "        print(\"Nearest Neigbours\")\n",
    "        sc.settings.n_jobs = n_jobs\n",
    "        from joblib import parallel_backend\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        with parallel_backend('threading', n_jobs=n_jobs): sc.pp.neighbors(adata, use_rep=use_rep)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    if umap:\n",
    "        print(\"UMAP\")\n",
    "        start_time = time.time()\n",
    "        sc.tl.umap(adata)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "    # print('before return')\n",
    "    # print(adata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chen_a',\n",
       " 'Chen_b',\n",
       " 'Chen_c',\n",
       " 'Hackney',\n",
       " 'Hafler',\n",
       " 'Roska',\n",
       " 'Wong',\n",
       " 'Scheetz',\n",
       " 'Sanes']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_names = ['Chen_a', 'Chen_b', 'Chen_c', 'Hackney', 'Hafler', 'Roska', 'Wong', 'Scheetz', 'Sanes']\n",
    "dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bkp = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add n of cells\n",
    "make_new_df = False\n",
    "if make_new_df:\n",
    "    rows = []\n",
    "    for ri, r in df_bkp.iterrows():\n",
    "        # for n_sample in [35000, 250000, 500000, 750000, 1000000, 1250000, 1500000, 1750000, 2000000] + [None]:\n",
    "        for n_sample in [None]:\n",
    "            for n_epochs in (10, 20, 50, 100,): # 20, 50, 100): # 5, 10, 15, 20, 25):\n",
    "                for n_layers in (3,):\n",
    "                    for n_hidden in (256,):\n",
    "                        from itertools import combinations\n",
    "                        added = set()\n",
    "                        for n_datasets in range(4, len(dataset_names) + 1):\n",
    "                            # print(n_datasets)\n",
    "                            for comb in combinations(set(dataset_names) - set(['Chen_a', 'Chen_b', 'Chen_c']), n_datasets):\n",
    "                                # if ('Roska' in comb or 'Hackney' in comb) and len(comb) + 3 < 8:\n",
    "                                #     continue\n",
    "                                if 'Roska' in comb and 'Hackney' in comb: #  and len(comb) + 3 == 8:\n",
    "                                    continue\n",
    "                                if 'Hackney' in comb:\n",
    "                                    continue\n",
    "                                if len(comb) != 5:\n",
    "                                    continue\n",
    "\n",
    "                                k = '_'.join(sorted(comb))\n",
    "                                print(k, comb)\n",
    "                                if not k in added:\n",
    "                                    added.add(k)\n",
    "                                else:\n",
    "                                    continue\n",
    "                                # print(k)\n",
    "                                # print(comb)\n",
    "\n",
    "                                for n_cells_roska in [125000]: # range(0, 175001, 25000):\n",
    "                                    rows.append(list(df_bkp.iloc[ri]) + [n_sample, n_epochs, comb, n_datasets + 3, n_cells_roska,\n",
    "                                                                        n_layers, n_hidden] )\n",
    "        break\n",
    "    df = pd.DataFrame(rows, columns=list(df_bkp.columns) + ['n.sample', 'n.epochs', 'comb', 'n.datasets', 'n_cells_roska', 'n_layers', 'n_hideen'])\n",
    "    # df['method'] = 'scanvi'\n",
    "    df['cell_type_key'] = 'cell.type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_new_df:\n",
    "    for ri, r in df.iterrows():\n",
    "        method, hvg, cell_type_key, input_path, output_path, in_exists, out_exists, n_sample, n_epochs, comb_add, n_datasets, n_cells_roska, n_layers, n_hidden = df.iloc[ri]\n",
    "        k_add = \"_\".join(comb_add)\n",
    "        output_path = output_path.replace('.h5ad', '_%s_%i_%s_NROSKA_%i_NEPOCHS_%i_NLAYERS_%i_NHIDDEN_%i.h5ad' % (str(n_sample), n_epochs, k_add, n_cells_roska, n_epochs, n_layers, n_hidden))\n",
    "        print(exists(output_path), output_path)\n",
    "    for ri, i in df.iterrows():\n",
    "        for rj, j in df.iterrows():\n",
    "            a, b = i['comb'], j['comb']\n",
    "            if ri == rj:\n",
    "                continue\n",
    "            if len(a) == len(b) and len(set(a) - set(b)) == 0:\n",
    "                print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 8, got 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-6c76b49ec9b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# method, hvg, cell_type_key, input_path, output_path, in_exists, out_exists, n_sample, n_epochs, comb_add, n_datasets, n_cells_roska, n_layers, n_hidden = df.iloc[query]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhvg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_type_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neurons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# method, hvg, cell_type_key, input_path, output_path,  in_exists, out_exists = df.iloc[query]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 8, got 6)"
     ]
    }
   ],
   "source": [
    "# if a for loop is approved, make a loop with increasing samples of cells\n",
    "import gc\n",
    "gc.collect()\n",
    "print('query', query)\n",
    "# method, hvg, cell_type_key, input_path, output_path, in_exists, out_exists, n_sample, n_epochs, comb_add, n_datasets, n_cells_roska, n_layers, n_hidden = df.iloc[query]\n",
    "\n",
    "method, hvg, cell_type_key, input_path, output_path, n_epochs, n_layers, n_hidden = df.iloc[query]\n",
    "# method, hvg, cell_type_key, input_path, output_path,  in_exists, out_exists = df.iloc[query]\n",
    "\n",
    "print(df.iloc[query])\n",
    "print('# epochs', n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[pd.Series([len(s) for s in df['comb']]) == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_add = \"_\".join(sorted(comb_add))\n",
    "\n",
    "# output_path = output_path.replace('.h5ad', '_%s_%i_%s_NROSKA_%i_NEPOCHS_%i_NLAYERS_%i_NHIDDEN_%i.h5ad' % (str(n_sample), n_epochs, k_add, n_cells_roska, n_epochs, n_layers, n_hidden))\n",
    "# output_path = output_path.replace('scvi', 'scanvi')\n",
    "\n",
    "print(exists(input_path), input_path)\n",
    "print(exists(output_path), output_path)\n",
    "print('')\n",
    "\n",
    "embed_path = output_path.replace('.h5ad', '_embed.csv')\n",
    "print(exists(embed_path), embed_path)\n",
    "\n",
    "parent_directory = os.path.abspath(os.path.join(output_path, os.pardir))\n",
    "# print(exists(parent_directory), parent_directory)\n",
    "if not exists(parent_directory):\n",
    "    os.makedirs(parent_directory)\n",
    "\n",
    "if exists(output_path):\n",
    "    print('this output file already exists. Modestly walking out...')\n",
    "    sys.exit()\n",
    "\n",
    "print('reading input...')\n",
    "ad = sc.read_h5ad(input_path)\n",
    "adata = ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata = adata[adata.obs.dataset.isin(set(comb_add)) | adata.obs.dataset.isin(set(['Chen_a','Chen_b', 'Chen_c'])),:]\n",
    "# print(adata.shape)\n",
    "# batch = 'batch_donor_dataset'\n",
    "\n",
    "# # adata = adata[adata.obs.dataset.isin(set(list(adata.obs.dataset.value_counts().index))),:]\n",
    "# gc.collect()\n",
    "\n",
    "# if n_sample is not None:\n",
    "#     import random\n",
    "#     random.seed(500)\n",
    "#     sel_names = pd.Series(adata.obs_names).sample(int(n_sample)).values\n",
    "#     adata = adata[adata.obs_names.isin(sel_names),:]\n",
    "\n",
    "    \n",
    "# idx_roska = adata[(adata.obs['dataset'] == 'Roska')].obs_names\n",
    "# idx_roska = pd.Series(idx_roska).sample(min(n_cells_roska, len(idx_roska)), random_state=500).values\n",
    "# print(len(idx_roska))\n",
    "\n",
    "# # downsample Roska\n",
    "# adata = adata[((adata.obs['dataset'] == 'Roska') & adata.obs_names.isin(set(idx_roska))) | ~adata.obs['dataset'].str.contains('Roska'),:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('location of scripts...')\n",
    "print(scIB.integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 1000)\n",
      "scanvi\n",
      "\u001b[34mINFO    \u001b[0m Dense size under 1Gb, casting to dense format \u001b[1m(\u001b[0mnp.ndarray\u001b[1m)\u001b[0m.                         \n",
      "\u001b[34mINFO    \u001b[0m Remapping batch_indices to \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m,N\u001b[1m]\u001b[0m                                                    \n",
      "\u001b[34mINFO    \u001b[0m Remapping labels to \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m,N\u001b[1m]\u001b[0m                                                           \n",
      "\u001b[34mINFO    \u001b[0m Computing the library size for the new data                                         \n",
      "\u001b[34mINFO    \u001b[0m Downsampled from \u001b[1;36m35000\u001b[0m to \u001b[1;36m35000\u001b[0m cells                                               \n",
      "scANVI dataset object with 107 batches and 11 cell types\n",
      "# epochs scVI 20\n",
      "# epochs scANVI 20\n",
      "\u001b[34mINFO    \u001b[0m KL warmup phase exceeds overall training phaseIf your applications rely on the      \n",
      "         posterior quality, consider training for more epochs or reducing the kl warmup.     \n",
      "\u001b[34mINFO    \u001b[0m KL warmup for \u001b[1;36m400\u001b[0m epochs                                                            \n",
      "training: 100%|██████████| 20/20 [05:34<00:00, 16.71s/it]\n",
      "\u001b[34mINFO    \u001b[0m Training is still in warming up phase. If your applications rely on the posterior   \n",
      "         quality, consider training for more epochs or reducing the kl warmup.               \n",
      "\u001b[34mINFO    \u001b[0m KL warmup phase exceeds overall training phaseIf your applications rely on the      \n",
      "         posterior quality, consider training for more epochs or reducing the kl warmup.     \n",
      "\u001b[34mINFO    \u001b[0m KL warmup for \u001b[1;36m400\u001b[0m epochs                                                            \n",
      "training: 100%|██████████| 20/20 [14:46<00:00, 44.35s/it]\n",
      "\u001b[34mINFO    \u001b[0m Training is still in warming up phase. If your applications rely on the posterior   \n",
      "         quality, consider training for more epochs or reducing the kl warmup.               \n",
      "about to write to output scANVI...\n",
      "Preparing dataset...\n",
      "setting up HVGs\n",
      "PCA and/or neighbors\n",
      "Nearest Neigbours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/ignacio.ibarra/miniconda3/envs/scIB-python/lib/python3.7/site-packages/numba/typed_passes.py:271: NumbaPerformanceWarning: \u001b[1m\n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\u001b[1m\n",
      "File \"../../../../../../../../../../../home/icb/ignacio.ibarra/miniconda3/envs/scIB-python/lib/python3.7/site-packages/umap/rp_tree.py\", line 135:\u001b[0m\n",
      "\u001b[1m@numba.njit(fastmath=True, nogil=True, parallel=True)\n",
      "\u001b[1mdef euclidean_random_projection_split(data, indices, rng_state):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n",
      "/home/icb/ignacio.ibarra/miniconda3/envs/scIB-python/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: \u001b[1m\u001b[1m\n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\u001b[1m\n",
      "File \"../../../../../../../../../../../home/icb/ignacio.ibarra/miniconda3/envs/scIB-python/lib/python3.7/site-packages/umap/utils.py\", line 409:\u001b[0m\n",
      "\u001b[1m@numba.njit(parallel=True)\n",
      "\u001b[1mdef build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  current_graph, n_vertices, n_neighbors, max_candidates, rng_state\n",
      "/home/icb/ignacio.ibarra/miniconda3/envs/scIB-python/lib/python3.7/site-packages/numba/typed_passes.py:271: NumbaPerformanceWarning: \u001b[1m\n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\u001b[1m\n",
      "File \"../../../../../../../../../../../home/icb/ignacio.ibarra/miniconda3/envs/scIB-python/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\u001b[0m\n",
      "\u001b[1m    @numba.njit(parallel=True)\n",
      "\u001b[1m    def nn_descent(\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 29.1847083568573 seconds ---\n",
      "after return\n",
      "Calculating UMAP...\n",
      "done...\n",
      "Saving embedding coordinates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/ignacio.ibarra/miniconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/plotting/_utils.py:314: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  pl.show()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54477"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(adata.shape)\n",
    "\n",
    "cell_type_key = 'cell.type'\n",
    "batch = 'batch_donor_dataset'\n",
    "\n",
    "integrated = None\n",
    "\n",
    "method = method.split('_')[0]\n",
    "# adata = scIB.integration.runScGen(adata, batch, labels)\n",
    "if method == 'scvi':\n",
    "    print('scVI...')\n",
    "    integrated, trainer = scIB.integration.runScvi(adata, batch, n_epochs=n_epochs)\n",
    "elif method == 'scgen':\n",
    "    print('scgen')\n",
    "    from tensorflow.python.client import device_lib\n",
    "    devices = device_lib.list_local_devices()\n",
    "    print('# of detected devices', len(devices))\n",
    "\n",
    "    gpu_found = False\n",
    "    device_gpu = '0'\n",
    "    for di, device in enumerate(devices):\n",
    "        if device.name.split(':')[1] == 'GPU':\n",
    "            gpu_found = True\n",
    "            device_gpu = str(di)\n",
    "        else:\n",
    "            if device.name.split(':')[1] == 'XLA_GPU' and not gpu_found:\n",
    "                device_gpu = str(di)\n",
    "        print(di, device.name)\n",
    "    print('GPU device found', gpu_found, di)\n",
    "\n",
    "    if not gpu_found:\n",
    "        print('Maybe XLA_GPU but not GPU found. Check tensorflow version')\n",
    "        # assert False\n",
    "\n",
    "    print(di, device.name)\n",
    "    batch = 'batch_donor_dataset'\n",
    "    print('starting scgen')\n",
    "\n",
    "    print('here...')\n",
    "    integrated = scIB.integration.runScGen_v2_0_0(adata, batch, cell_type_key, epochs=n_epochs, device=device_gpu, verbose=1)\n",
    "elif method == 'scanvi':\n",
    "    print('scanvi')\n",
    "    # scvi\n",
    "    integrated = scIB.integration.runScanvi(adata, batch, cell_type_key, n_epochs_scVI=n_epochs, n_epochs_scANVI=25,\n",
    "                                            n_layers=int(n_layers), n_hidden=int(n_hidden))\n",
    "    print ('about to write to output scANVI...')\n",
    "    # assert False\n",
    "\n",
    "\n",
    "from os.path import exists\n",
    "if integrated is not None and not exists(output_path):\n",
    "    sc.write(output_path, integrated)\n",
    "\n",
    "sys.exit()\n",
    "\n",
    "adata = integrated\n",
    "result = 'embed'\n",
    "method = 'scanvi'\n",
    "\n",
    "print('Preparing dataset...')\n",
    "if result == 'embed':\n",
    "    reduce_data(adata, n_top_genes=adata.shape[1], neighbors=True,\n",
    "                use_rep='X_emb', pca=False, umap=False)\n",
    "elif result == 'full':\n",
    "    sc.pp.filter_genes(adata, min_cells=1)\n",
    "    reduce_data(adata, n_top_genes=adata.shape[1], neighbors=True,\n",
    "                use_rep='X_pca', pca=True, umap=False)\n",
    "\n",
    "print('after return')\n",
    "# print(adata)\n",
    "# Calculate embedding\n",
    "if method.startswith('conos'):\n",
    "    print('Calculating graph embedding...')\n",
    "    sc.tl.draw_graph(adata, key_added_ext='graph')\n",
    "    basis = 'draw_graph_graph'\n",
    "    label = 'Graph'\n",
    "else:\n",
    "    print('Calculating UMAP...')\n",
    "    sc.tl.umap(adata)\n",
    "    basis = 'umap'\n",
    "    label = 'UMAP'\n",
    "\n",
    "print('done...')\n",
    "import os\n",
    "# Save embedding coordinates\n",
    "print('Saving embedding coordinates...')\n",
    "label = 'UMAP'\n",
    "basis = 'umap'\n",
    "adata.obs[label + '1'] = adata.obsm['X_' + basis][:, 0]\n",
    "adata.obs[label + '2'] = adata.obsm['X_' + basis][:, 1]\n",
    "coords = adata.obs[['cell.type', 'batch_donor_dataset', label + '1', label + '2' ]]\n",
    "coords.to_csv(os.path.join(embed_path), index_label='CellID')\n",
    "\n",
    "sc.set_figure_params(facecolor='white', dpi=150)\n",
    "sc.pl.umap(adata, color=['cell.type', 'dataset'])\n",
    "plt.savefig(output_path.replace('.h5ad', '.png'))\n",
    "plt.close()\n",
    "\n",
    "print(exists(output_path), output_path)\n",
    "\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# elbo_train_set = trainer.history[\"elbo_train_set\"]\n",
    "# elbo_test_set = trainer.history[\"elbo_test_set\"]\n",
    "# x = np.linspace(0, n_epochs, (len(elbo_train_set)))\n",
    "# plt.plot(x, elbo_train_set, label=\"train\")\n",
    "# plt.plot(x, elbo_test_set, label=\"test\")\n",
    "# # plt.ylim(1500, 3000)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(integrated)\n",
    "# print(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('done...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scIB-python]",
   "language": "python",
   "name": "conda-env-scIB-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
