{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scanpy as sc\n",
    "from os.path import join, exists\n",
    "from os import listdir\n",
    "import anndata\n",
    "import scipy\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# convert counts into float32\n",
    "# Convenience method for computing the size of objects\n",
    "def print_size_in_MB(x):\n",
    "    return '{:.3} MB'.format(x.__sizeof__()/1e6)\n",
    "\n",
    "def print_size_in_MB_sparse_matrix(a):\n",
    "    # a = scipy.sparse.csr_matrix(np.random.randint(10, size=(40, 3)))\n",
    "    # x = a.data.nbytes + a.indptr.nbytes + a.indices.nbytes\n",
    "    size = a.data.size/(1024**2)\n",
    "    return '{:.3} MB'.format(size)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "for n_sample_per_batch in [500, 1000, None]:\n",
    "    # examine types, columns and others incorporated in the object\n",
    "    \n",
    "    code_n_cells = (('_' + str(n_sample_per_batch) if n_sample_per_batch is not None else ''))\n",
    "\n",
    "    print(code_n_cells)\n",
    "\n",
    "    print('# of cells (input argument)', n_sample_per_batch)\n",
    "    \n",
    "    code_output = (('_' + str(n_sample_per_batch) if n_sample_per_batch is not None else '_all'))\n",
    "    output_path = '../../data/integration_march_2021/input/input%s_cells.h5ad' % code_output\n",
    "    print(output_path)            \n",
    "    \n",
    "    if exists(output_path) and not overwrite:\n",
    "        continue\n",
    "\n",
    "    dataset_names = [\"Wong\", \"Scheetz\", \"Chen_c\", \"Hafler\", \"Roska\", \"Chen_a\", \"Sanes\", \"Hackney\", \"Chen_b\"]\n",
    "\n",
    "    # to avoid memory leaks do it in two rounds\n",
    "    p1 = output_path.replace('.h5ad', '_part1.h5ad')\n",
    "\n",
    "    if not exists(p1):\n",
    "        print('loading', dataset_names[:4])\n",
    "        ad1 = get_datasets(dataset_names[:4], code_n_cells=code_n_cells)\n",
    "\n",
    "        print('ad1')\n",
    "        print ('laoding datasets 1 done...')\n",
    "        print(ad1.obs.dataset.value_counts())\n",
    "        # save part1\n",
    "         # save part1\n",
    "        ad1 = ad1[ad1.obs.dataset.isin(set(dataset_names[:4])),:]\n",
    "        ad1.write(p1, compression='lzf')\n",
    "        del ad1\n",
    "        print(p1)\n",
    "\n",
    "    p2 = output_path.replace('.h5ad', '_part2.h5ad')\n",
    "    if not exists(p2):\n",
    "        print('loading', dataset_names[4:])\n",
    "        ad2 = get_datasets(dataset_names[4:], code_n_cells=code_n_cells)\n",
    "        print('ad2')\n",
    "        print(ad2)\n",
    "        print(ad2.obs.index)\n",
    "        print ('laoding datasets 2 done...')\n",
    "        print(ad2.obs.dataset.value_counts())\n",
    "\n",
    "        # save part1\n",
    "        ad2 = ad2[ad2.obs.dataset.isin(set(dataset_names[4:])),:]\n",
    "        ad2.write(p2, compression='lzf')\n",
    "        del ad2\n",
    "        print(p2)           \n",
    "\n",
    "    ad1 = sc.read_h5ad(p1)\n",
    "    ad2 = sc.read_h5ad(p2)\n",
    "\n",
    "    # print(ad1.obs.dataset.value_counts())\n",
    "    # print(ad2.obs.dataset.value_counts())\n",
    "\n",
    "    print('concatenating...')\n",
    "    ad_final = anndata.concat([ad1, ad2])\n",
    "    print('done...')\n",
    "\n",
    "    print('ad final')\n",
    "    print(ad1.shape, ad2.shape)\n",
    "    print(ad_final.shape)\n",
    "    # print(ad_final.obs.index)\n",
    "\n",
    "    # define a unified code for all categories\n",
    "    ad_final.obs['batch.merged'] = ad_final.obs['dataset'].astype(str) + ':' + ad_final.obs['batch'].astype(str)\n",
    "    ad_final.obs['batch.merged'] = ad_final.obs['batch.merged'].astype('category').cat.codes\n",
    "    # input_scib.obs['batch.merged'].value_counts()\n",
    "    ad_final.obs['batch.merged'] = ad_final.obs['batch.merged'].astype('category').astype(str)\n",
    "    # print(ad_final.obs['batch.merged'].value_counts())\n",
    "\n",
    "    # we only care about genes detected in at least X cells or more (X=50)\n",
    "    # min_cells = 50\n",
    "    # sc.pp.filter_genes(ad, min_cells=min_cells)\n",
    "\n",
    "\n",
    "    ad_final = ad_final[ad_final.obs['batch.merged'].map(ad_final.obs['batch.merged'].value_counts().to_dict()) > 100,:]\n",
    "    ad_final.obs['batch.merged'].value_counts()\n",
    "\n",
    "    print('saving to output...')\n",
    "    ad_final.write(output_path, compression='lzf')\n",
    "    print('done...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mypython3] *",
   "language": "python",
   "name": "conda-env-mypython3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
