{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group samples by dataset, using increasing numbers of cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scIB\n",
    "import os\n",
    "import scanpy as sc\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "import anndata\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "\n",
    "# convert counts into float32\n",
    "# Convenience method for computing the size of objects\n",
    "def print_size_in_MB(x):\n",
    "    print('{:.3} MB'.format(x.__sizeof__()/1e6))\n",
    "\n",
    "### Use the scran related directory to map all the files we need to put together.\n",
    "datadir_orig = '/storage/groups/ml01/datasets/projects/20210318_retinal_data_integration_ignacio.ibarra_malte.luecken'\n",
    "datadir_scran = '/mnt/znas/icb_zstore01/groups/ml01/workspace/ignacio.ibarra/theislab/retinal_scRNAseq_integration/data/integration_march_2021/scran'\n",
    "filenames = [f for f in os.listdir(datadir_orig)]\n",
    "filenames_md5 = [f.strip() for f in open(os.path.join(datadir_orig, 'md5sum.txt'))]\n",
    "\n",
    "files = set()\n",
    "for qi in filenames_md5:\n",
    "    md5, fi = qi.split('  ')\n",
    "    # print(fi)\n",
    "    found = os.path.exists(os.path.join(datadir_scran, fi))\n",
    "    if not found:\n",
    "        print('not found', fi)\n",
    "    files.add(fi)\n",
    "\n",
    "**The following files are listed but for some reason not found anymore. Consider deleting (Request to Jin first)**\n",
    "\n",
    "filenames_by_dataset = {}\n",
    "for f in filenames_md5:\n",
    "    dataset, filename = f.split(' ')[-1].split('/')[-2:]\n",
    "    if not dataset in filenames_by_dataset:\n",
    "        filenames_by_dataset[dataset] = []\n",
    "    filenames_by_dataset[dataset].append(filename)\n",
    "\n",
    "# get all files from a single directory\n",
    "def get_by_dataset(dataset_name, filenames=None, n_sample=None):\n",
    "    adatas = []\n",
    "    \n",
    "    if (filenames is None):\n",
    "        filenames = [f for f in listdir(join(datadir_scran, dataset_name))]\n",
    "    print('# datasets', len(filenames))\n",
    "    for f in filenames:\n",
    "        if len(adatas) % 20 == 0:\n",
    "            print('loaded so far', len(adatas))\n",
    "        p = join(datadir_scran, dataset_name, f)\n",
    "        print(p)\n",
    "        ad = sc.read_h5ad(p)\n",
    "        \n",
    "        if n_sample is not None:\n",
    "            idx_sample = ad.obs.sample(n_sample if n_sample < ad.shape[0] else ad.shape[0]).index\n",
    "            ad = ad[ad.obs.index.isin(idx_sample),:]\n",
    "            # print(ad.shape)        \n",
    "        \n",
    "        ad.obs['dataset'] = dataset_name\n",
    "        ad.obs['filename'] = f.replace('.h5ad', '')\n",
    "        adatas.append(ad)\n",
    "    return adatas[0].concatenate(adatas[1:]) # join='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wong\n",
      "Scheetz\n",
      "Chen_c\n",
      "Hafler\n",
      "Roska\n",
      "Chen_a\n",
      "Sanes\n",
      "Hackney\n",
      "Chen_b\n",
      "Wong\n",
      "Scheetz\n",
      "Chen_c\n",
      "Hafler\n",
      "Roska\n",
      "Chen_a\n",
      "Sanes\n",
      "Hackney\n",
      "Chen_b\n",
      "Wong\n",
      "Scheetz\n",
      "Chen_c\n",
      "Hafler\n",
      "Roska\n",
      "Chen_a\n",
      "False ../../data/integration_march_2021/input/bydataset/Chen_a.h5ad\n",
      "Sanes\n",
      "Hackney\n",
      "Chen_b\n"
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "for n_sample in [250, 500, 1000, None]:\n",
    "    for dataset in filenames_by_dataset:\n",
    "        print(dataset)\n",
    "\n",
    "        subsampling_code = ('_' + str(n_sample) if n_sample is not None else '')\n",
    "        next_filename = '%s%s.h5ad' % (dataset, subsampling_code)\n",
    "        outdir = '../../data/integration_march_2021/input/bydataset%s' % subsampling_code\n",
    "        if not exists(outdir):\n",
    "            os.mkdir(outdir)\n",
    "        path_by_dataset = join(outdir, '%s' % (next_filename))\n",
    "\n",
    "        \n",
    "        if exists(path_by_dataset):\n",
    "            continue\n",
    "            \n",
    "        print(exists(path_by_dataset), path_by_dataset)\n",
    "        continue\n",
    "            \n",
    "        ad = get_by_dataset(dataset, filenames=filenames_by_dataset[dataset])\n",
    "        \n",
    "        if n_sample is not None:\n",
    "            sel_idx = ad.obs.groupby('batch').apply(lambda x: x.sample(min(n_sample, len(x)))).index.get_level_values(None)\n",
    "            ad = ad[ad.obs.index.isin(sel_idx),:]\n",
    "            # print(ad.obs.batch.value_counts())\n",
    "            print(ad.shape)\n",
    "        \n",
    "        print(ad.shape)\n",
    "        ad.write(path_by_dataset, compression='lzf')\n",
    "        print(dataset, 'done...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scIB-python]",
   "language": "python",
   "name": "conda-env-scIB-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
