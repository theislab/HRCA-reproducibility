{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scran normalization step (per sample)\n",
    "- All the samples are processed independently, and we generated scran samples for each of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scanpy as sc\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "import anndata\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert counts into float32\n",
    "# Convenience method for computing the size of objects\n",
    "def print_size_in_MB(x):\n",
    "    print('{:.3} MB'.format(x.__sizeof__()/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/storage/groups/ml01/datasets/projects/20210318_retinal_data_integration_ignacio.ibarra_malte.luecken'\n",
    "outdir = '/mnt/znas/icb_zstore01/groups/ml01/workspace/ignacio.ibarra/theislab/retinal_scRNAseq_integration/data/integration_march_2021/scran'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [f for f in os.listdir(datadir)]\n",
    "filenames_md5 = [r.strip() for f in ['md5sum_chen_5_fix5.txt', 'md5sum.txt'] for r in open(os.path.join(datadir, f))]\n",
    "\n",
    "files = set()\n",
    "for qi in filenames_md5:\n",
    "    md5, fi = qi.split('  ')\n",
    "    found = os.path.exists(os.path.join(datadir, fi))\n",
    "    if not found:\n",
    "        print('not found', fi)\n",
    "    files.add(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_by_dataset = {}\n",
    "for f in filenames_md5:\n",
    "    dataset, filename = f.split(' ')[-1].split('/')[-2:]\n",
    "    if not dataset in filenames_by_dataset:\n",
    "        filenames_by_dataset[dataset] = []\n",
    "    filenames_by_dataset[dataset].append(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scran normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Chen_a_fix5', 'Wong', 'Scheetz', 'Roska', 'Chen_c', 'Hafler', 'Chen_a', 'Sanes', 'Hackney', 'Chen_b'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames_by_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "from multiprocessing import Manager\n",
    "\n",
    "def execute_preprocessing(input_path, output_path):\n",
    "    print('')\n",
    "    path_preprocessing = '../../scib/scripts/preprocessing_remove_empty.py'\n",
    "    cmd = 'python %s -i %s -o %s' % (path_preprocessing, input_path, output_path)\n",
    "    \n",
    "    try:\n",
    "        print(cmd)\n",
    "        os.system(cmd)\n",
    "    except Exception as err:\n",
    "        print('something went wrong...')\n",
    "        print(err)\n",
    "\n",
    "def run(function, input_list, n_cores, log_each=None, log=False):\n",
    "    print(('run function %s with n_cores = %i' % (function, n_cores)))\n",
    "    print(function)\n",
    "    # print 'with input list of len'\n",
    "    # print len(input_list)\n",
    "    # print 'in groups of %d threads' % n_threads\n",
    "\n",
    "    assert n_cores <= 20\n",
    "\n",
    "    # the type of input_list has to be a list. If not\n",
    "    # then it can a single element list and we cast it to list.\n",
    "    if not isinstance(type(input_list[0]), type(list)):\n",
    "        input_list = [[i] for i in input_list]\n",
    "\n",
    "    n_groups = int(len(input_list) / n_cores + 1)\n",
    "    # print 'n groups', n_groups\n",
    "\n",
    "    n_done = 0\n",
    "    for group_i in range(n_groups):\n",
    "        start, end = group_i * n_cores, (group_i + 1) * n_cores\n",
    "        # print 'start', start, 'end', end\n",
    "\n",
    "        threads = [None] * (end - start)\n",
    "        for i, pi in enumerate(range(start, min(end, len(input_list)))):\n",
    "            next_args = input_list[pi]\n",
    "            if log:\n",
    "                print(next_args)\n",
    "            # print next_kmer\n",
    "            threads[i] = Process(target=function, args=next_args)\n",
    "            # print 'starting process #', i\n",
    "            threads[i].start()\n",
    "\n",
    "        # print  threads\n",
    "        # print 'joining threads...'\n",
    "        # do some other stuff\n",
    "        for i in range(len(threads)):\n",
    "            if threads[i] is None:\n",
    "                continue\n",
    "            threads[i].join()\n",
    "\n",
    "            n_done += 1\n",
    "            if log_each is not None and log_each % n_done == 0:\n",
    "                print('Done %i so far' % n_done)\n",
    "    print('done...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "arguments = []\n",
    "\n",
    "for dataset in filenames_by_dataset:\n",
    "    # print(dataset)\n",
    "    for filename in filenames_by_dataset[dataset]:\n",
    "        input_file = join(datadir, dataset, filename)\n",
    "        next_outdir = join(outdir, dataset)\n",
    "        # print(next_outdir)\n",
    "        \n",
    "        if not os.path.exists(next_outdir):\n",
    "            os.mkdir(next_outdir)\n",
    "            \n",
    "        output_file = join(next_outdir, filename)\n",
    "\n",
    "        # print(os.path.exists(output_file), output_file)\n",
    "        if os.path.exists(output_file):\n",
    "            # print(os.path.exists(output_file), 'skip...')\n",
    "            continue\n",
    "        \n",
    "        # print(input_file)\n",
    "        # print(output_file)\n",
    "        # print('')\n",
    "        \n",
    "        arguments.append([input_file, output_file])\n",
    "        # ad = sc.read_h5ad(join(datadir, p))\n",
    "\n",
    "arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a test. maintain commented after finishing\n",
    "# !python ../../scib/scripts/preprocessing_remove_empty.py -i /storage/groups/ml01/datasets/projects/20210318_retinal_data_integration_ignacio.ibarra_malte.luecken/Wong/Retina_2B.h5ad -o /mnt/znas/icb_zstore01/groups/ml01/workspace/ignacio.ibarra/theislab/retinal_scRNAseq_integration/data/integration_march_2021/scran/Wong/Retina_2B.h5ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(arguments))\n",
    "arguments = sorted(arguments, key=lambda x: os.path.getsize(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run function <function execute_preprocessing at 0x7f64de357440> with n_cores = 10\n",
      "<function execute_preprocessing at 0x7f64de357440>\n",
      "\n",
      "python ../../scib/scripts/preprocessing_remove_empty.py -i /storage/groups/ml01/datasets/projects/20210318_retinal_data_integration_ignacio.ibarra_malte.luecken/Chen_a_fix5/10x_Lobe_D28_13_Nu.h5ad -o /mnt/znas/icb_zstore01/groups/ml01/workspace/ignacio.ibarra/theislab/retinal_scRNAseq_integration/data/integration_march_2021/scran/Chen_a_fix5/10x_Lobe_D28_13_Nu.h5ad\n",
      "\n",
      "python ../../scib/scripts/preprocessing_remove_empty.py -i /storage/groups/ml01/datasets/projects/20210318_retinal_data_integration_ignacio.ibarra_malte.luecken/Chen_a_fix5/10x_Lobe_D27_13_Nu.h5ad -o /mnt/znas/icb_zstore01/groups/ml01/workspace/ignacio.ibarra/theislab/retinal_scRNAseq_integration/data/integration_march_2021/scran/Chen_a_fix5/10x_Lobe_D27_13_Nu.h5ad\n",
      "\n",
      "python ../../scib/scripts/preprocessing_remove_empty.py -i /storage/groups/ml01/datasets/projects/20210318_retinal_data_integration_ignacio.ibarra_malte.luecken/Chen_a_fix5/10x_Lobe_D30_13_NeuN.h5ad -o /mnt/znas/icb_zstore01/groups/ml01/workspace/ignacio.ibarra/theislab/retinal_scRNAseq_integration/data/integration_march_2021/scran/Chen_a_fix5/10x_Lobe_D30_13_NeuN.h5ad\n",
      "\n",
      "python ../../scib/scripts/preprocessing_remove_empty.py -i /storage/groups/ml01/datasets/projects/20210318_retinal_data_integration_ignacio.ibarra_malte.luecken/Chen_a_fix5/10x3_Lobe_D005_13_NeuN.h5ad -o /mnt/znas/icb_zstore01/groups/ml01/workspace/ignacio.ibarra/theislab/retinal_scRNAseq_integration/data/integration_march_2021/scran/Chen_a_fix5/10x3_Lobe_D005_13_NeuN.h5ad\n",
      "\n",
      "python ../../scib/scripts/preprocessing_remove_empty.py -i /storage/groups/ml01/datasets/projects/20210318_retinal_data_integration_ignacio.ibarra_malte.luecken/Chen_a_fix5/10x3_Lobe_D013_13_NeuN.h5ad -o /mnt/znas/icb_zstore01/groups/ml01/workspace/ignacio.ibarra/theislab/retinal_scRNAseq_integration/data/integration_march_2021/scran/Chen_a_fix5/10x3_Lobe_D013_13_NeuN.h5ad\n",
      "done...\n"
     ]
    }
   ],
   "source": [
    "run(execute_preprocessing, arguments, n_cores=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done...\n"
     ]
    }
   ],
   "source": [
    "print('done...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scIB-python]",
   "language": "python",
   "name": "conda-env-scIB-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
