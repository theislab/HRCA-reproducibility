{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cdcb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scanpy as sc\n",
    "from os.path import join, exists\n",
    "from os import listdir\n",
    "import anndata\n",
    "import scipy\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# convert counts into float32\n",
    "# Convenience method for computing the size of objects\n",
    "def print_size_in_MB(x):\n",
    "    return '{:.3} MB'.format(x.__sizeof__()/1e6)\n",
    "\n",
    "def print_size_in_MB_sparse_matrix(a):\n",
    "    # a = scipy.sparse.csr_matrix(np.random.randint(10, size=(40, 3)))\n",
    "    # x = a.data.nbytes + a.indptr.nbytes + a.indices.nbytes\n",
    "    size = a.data.size/(1024**2)\n",
    "    return '{:.3} MB'.format(size)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('here...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea146a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [['Hackney', 'Roska', 'Hafler', 'Wong', 'Scheetz', 'Chen_b', 'Chen_c', 'Sanes', 'Chen_a'],\n",
    "                ['Chen_b', 'Chen_c', 'Chen_a']]\n",
    "    \n",
    "dataset_codes = ['all', 'Chen']\n",
    "# add Chen_a plus all others\n",
    "for k in combinations[0]:\n",
    "    if 'Chen' in k:\n",
    "        continue\n",
    "    combinations.append(combinations[1] + [k])\n",
    "    dataset_codes.append('Chen+%s' % k)\n",
    "\n",
    "for x, y in zip(dataset_codes, combinations):\n",
    "    print(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef3cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [['Hackney', 'Roska', 'Hafler', 'Wong', 'Scheetz', 'Chen_b', 'Chen_c', 'Sanes', 'Chen_a'],\n",
    "                ['Chen_b', 'Chen_c', 'Chen_a']]\n",
    "    \n",
    "dataset_codes = ['all', 'Chen']\n",
    "# add Chen_a plus all others\n",
    "for k in combinations[0]:\n",
    "    if 'Chen' in k:\n",
    "        continue\n",
    "    combinations.append(combinations[1] + [k])\n",
    "    dataset_codes.append('Chen+%s' % k)\n",
    "\n",
    "for x, y in zip(dataset_codes, combinations):\n",
    "    print(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133e97f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad1 = sc.read_h5ad(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df5286",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9d52f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "overwrite = False\n",
    "for n_sample_per_batch in [500]: # , 500, None]:\n",
    "    # if n_sample_per_batch != None:\n",
    "    #    continue\n",
    "    if n_sample_per_batch != None and n_sample_per_batch != 500:\n",
    "        continue\n",
    "    # examine types, columns and others incorporated in the object\n",
    "    \n",
    "    code_n_cells = (('_' + str(n_sample_per_batch) if n_sample_per_batch is not None else ''))\n",
    "\n",
    "    print(code_n_cells)\n",
    "\n",
    "    print('# of cells (input argument)', n_sample_per_batch)\n",
    "    \n",
    "    code_output = (('_' + str(n_sample_per_batch) if n_sample_per_batch is not None else '_all'))\n",
    "\n",
    "    for dataset_names_subset, dataset_code in zip(combinations, dataset_codes):\n",
    "        \n",
    "        output_path = '../../data/integration_march_2021/input/input%s_cells_%s.h5ad' % (code_output, dataset_code)\n",
    "        print(exists(output_path), output_path)\n",
    "        \n",
    "        if exists(output_path):\n",
    "            continue\n",
    "        \n",
    "        print(dataset_code, dataset_names_subset)\n",
    "        p1 = output_path.replace('.h5ad', '_part1.h5ad')\n",
    "\n",
    "        names1 = dataset_names_subset[:4]\n",
    "        names2 = dataset_names_subset[4:-1]\n",
    "        names3 = dataset_names_subset[-1:]\n",
    "        \n",
    "        if dataset_code != 'all':\n",
    "            names1, names2, names3 = names1, [], []\n",
    "        print(names1)\n",
    "        print(names2)\n",
    "        print(names3)\n",
    "                \n",
    "        if not exists(p1) and len(names1) > 0:\n",
    "            ad1 = get_datasets(names1, code_n_cells=code_n_cells)\n",
    "\n",
    "            print('ad1')\n",
    "            print ('laoding datasets 1 done...')\n",
    "            print(ad1.obs.dataset.value_counts())\n",
    "            # save part1\n",
    "             # save part1\n",
    "            ad1 = ad1[ad1.obs.dataset.isin(set(names1)),:]\n",
    "            ad1.write(p1, compression='lzf')\n",
    "            del ad1\n",
    "            print(p1)\n",
    "\n",
    "        p2 = output_path.replace('.h5ad', '_part2.h5ad')\n",
    "        if not exists(p2) and len(names2) > 0:\n",
    "            print('loading', names2)\n",
    "            ad2 = get_datasets(names2, code_n_cells=code_n_cells)\n",
    "            print('ad2')\n",
    "            print(ad2)\n",
    "            print(ad2.obs.index)\n",
    "            print ('laoding datasets 2 done...')\n",
    "            print(ad2.obs.dataset.value_counts())\n",
    "\n",
    "            # save part1\n",
    "            ad2 = ad2[ad2.obs.dataset.isin(set(names2)),:]\n",
    "            ad2.write(p2, compression='lzf')\n",
    "            del ad2\n",
    "            print(p2)   \n",
    "\n",
    "        p3 = output_path.replace('.h5ad', '_part3.h5ad')\n",
    "        if not exists(p3) and len(names3) > 0:\n",
    "            print('loading', names3)\n",
    "            ad3 = get_datasets(names3, code_n_cells=code_n_cells)\n",
    "            print('ad2')\n",
    "            print(ad3)\n",
    "            print(ad3.obs.index)\n",
    "            print ('laoding datasets 3 done...')\n",
    "            print(ad3.obs.dataset.value_counts())\n",
    "\n",
    "            ad3 = ad3[ad3.obs.dataset.isin(set(names3)),:]\n",
    "            ad3.write(p3, compression='lzf')\n",
    "            del ad3\n",
    "            print(p3)    \n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        ad1, ad2, ad3 = None, None, None\n",
    "        # filter: only the datasets subset can be in the object\n",
    "        ad1 = sc.read_h5ad(p1) #  cache=True)\n",
    "        ad1 = ad1[ad1.obs['dataset'].isin(set(dataset_names_subset))]\n",
    "        print(ad1.obs.dataset.value_counts())\n",
    "\n",
    "        if exists(p2) and exists(p3):\n",
    "            ad2 = sc.read_h5ad(p2) #  cache=True)\n",
    "            ad3 = sc.read_h5ad(p3) #  cache=True)\n",
    "            ad2 = ad2[ad2.obs['dataset'].isin(set(dataset_names_subset))]\n",
    "            ad3 = ad3[ad3.obs['dataset'].isin(set(dataset_names_subset))]\n",
    "            print(ad2.obs.dataset.value_counts())\n",
    "            print(ad3.obs.dataset.value_counts())\n",
    "            # print(ad2.obs.dataset.value_counts())\n",
    "\n",
    "        gc.collect()\n",
    "        print('concatenating...')\n",
    "        ad_final = anndata.concat([ad1, ad2, ad3]) if (ad2 is not None and ad3 is not None) else ad1\n",
    "\n",
    "\n",
    "        print(ad1.shape, ad2.shape, ad3.shape)\n",
    "        \n",
    "        gc.collect()\n",
    "        print('done...')\n",
    "\n",
    "        print('ad final')\n",
    "        # print(ad1.shape, ad2.shape)\n",
    "        print(ad_final.shape)\n",
    "        # print(ad_final.obs.index)\n",
    "\n",
    "        # define a unified code for all categories\n",
    "        ad_final.obs['batch.merged'] = ad_final.obs['dataset'].astype(str) + ':' + ad_final.obs['batch'].astype(str)\n",
    "        ad_final.obs['batch.merged'] = ad_final.obs['batch.merged'].astype('category').cat.codes\n",
    "        # input_scib.obs['batch.merged'].value_counts()\n",
    "        ad_final.obs['batch.merged'] = ad_final.obs['batch.merged'].astype('category').astype(str)\n",
    "        # print(ad_final.obs['batch.merged'].value_counts())\n",
    "        \n",
    "        # include the donor information\n",
    "        donor = pd.read_csv('data/donor_details.tsv', sep='\\t')\n",
    "        donor['k'] = donor['file'].str.replace('.', '').str.replace('h5ad', '')\n",
    "        donor['dataset'] = donor['k'].str.split('/').str[1]\n",
    "        donor['filename'] = donor['k'].str.split('/').str[2]\n",
    "        donor_by_filename = donor[['donor', 'filename']].set_index('filename')['donor'].to_dict()\n",
    "        ad_final.obs['donor'] = ad_final.obs['filename'].map(donor_by_filename)\n",
    "        \n",
    "        \n",
    "        ad_final.obs['batch_donor_dataset'] = ad_final.obs['donor'].astype(str) + ':' + ad_final.obs['dataset'].astype(str) + ':' + ad_final.obs['batch'].astype(str)\n",
    "\n",
    "        print('before batch filter (n=100)')\n",
    "        print(ad_final.shape)\n",
    "        ad_final = ad_final[ad_final.obs['batch_donor_dataset'].map(ad_final.obs['batch_donor_dataset'].value_counts().to_dict()) > 100,:]\n",
    "        ad_final.obs['batch_donor_dataset'].value_counts()\n",
    "\n",
    "        ad_final.obs['batch_donor_dataset'] = ad_final.obs['batch_donor_dataset'].astype('category')\n",
    "        \n",
    "        print('after batch filter (n=100)')\n",
    "        print(ad_final.shape)\n",
    "        print('saving to output...')\n",
    "        ad_final.write(output_path, compression='lzf')\n",
    "\n",
    "        if exists(p1):\n",
    "            os.remove(p1)\n",
    "        if exists(p2):\n",
    "            os.remove(p2)\n",
    "        if exists(p3):\n",
    "            os.remove(p3)\n",
    "        \n",
    "        print('done...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = sc.read_h5ad('/storage/groups/ml01/workspace/ignacio.ibarra/theislab/retinal_scRNAseq_integration/data/integration_march_2021/input/input_all_cells_Chen.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb093184",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy3",
   "language": "python",
   "name": "mypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
